{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointnet - CIFAR 10\n",
    "\n",
    "we will using cifar-10 as our dataset to do the classfication work\n",
    "\n",
    "Since the image are 1024(32x32) point only, we will choosing 512 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets\n",
    "\n",
    "### 1. Downdload datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "BASE_DIR = '/notebooks/'\n",
    "# Download dataset for point cloud classification\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "if not os.path.exists(os.path.join(DATA_DIR, 'cifar-10-python-py')):\n",
    "    www = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "\n",
    "    zipfile = os.path.basename(www)\n",
    "    os.system('wget --no-check-certificate  %s; tar -xzf %s' % (www, zipfile))\n",
    "    os.system('mv cifar-10-batches-py %s' % DATA_DIR+'/cifar-10-python-py')\n",
    "    os.system('rm cifar-10-python.tar.gz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. prepare h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Write numpy array data and label to h5_filename\n",
    "def save_h5_data_label(h5_filename, data, label, data_dtype='float32', label_dtype='uint8'):\n",
    "    h5_fout = h5py.File(h5_filename)\n",
    "    h5_fout.create_dataset(\n",
    "            'data', data=data,\n",
    "            compression='gzip', compression_opts=4,\n",
    "            dtype=data_dtype)\n",
    "    h5_fout.create_dataset(\n",
    "            'label', data=label,\n",
    "            compression='gzip', compression_opts=1,\n",
    "            dtype=label_dtype)\n",
    "    h5_fout.close()\n",
    "\n",
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batchs = [\n",
    "    'data/cifar-10-python-py/data_batch_1',\n",
    "    'data/cifar-10-python-py/data_batch_2',\n",
    "    'data/cifar-10-python-py/data_batch_3',\n",
    "    'data/cifar-10-python-py/data_batch_4',\n",
    "    'data/cifar-10-python-py/data_batch_5',\n",
    "]\n",
    "\n",
    "test_batch = [\n",
    "    'data/cifar-10-python-py/test_batch',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform2pointcloud(file_path):\n",
    "    \"\"\"\n",
    "    transfrom iamge to cloud points\n",
    "    \"\"\"\n",
    "    pickle = unpickle(file_path)\n",
    "    label = pickle['labels']\n",
    "    data = pickle['data']\n",
    "    \n",
    "    data_arr = data.reshape([-1, 3, 32, 32])\n",
    "    data_arr = np.transpose(data_arr, (0, 2, 3, 1))\n",
    "    data_arr = data_arr.astype(np.float32)\n",
    "    # normalize the data with -0.5, 0.5\n",
    "    data_arr = data_arr/255. - 0.5\n",
    "    data_arr_size = data_arr.shape\n",
    "    \n",
    "    data = np.zeros((data_arr_size[0], data_arr_size[1], data_arr_size[2], 3+3), np.float32)\n",
    "    data[:,:,:,3:6] = data_arr\n",
    "    \n",
    "    index_arr = np.array(list(np.ndindex((32,32)))).reshape((32,32,2)).astype(np.float32)/32\n",
    "    index_arr = np.expand_dims(index_arr, axis=0)\n",
    "    index_arr = np.repeat(index_arr, [data_arr_size[0]], axis=0)\n",
    "    data[:,:,:,0:2] = index_arr  \n",
    "    data = data.reshape((data_arr_size[0], data_arr_size[1]*data_arr_size[2],  (3+3)))\n",
    "    \n",
    "    return data, np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10000, 1024, 6), (10000,))\n",
      "(dtype('float32'), dtype('int64'))\n",
      "('x,y,z,r,g,b', 0.03125)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "data, label = transform2pointcloud(train_batchs[0])\n",
    "print(data.shape, label.shape)\n",
    "print(data.dtype, label.dtype)\n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.imshow(data[4])\n",
    "print('x,y,z,r,g,b', data[4,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cifar-10-python-py/batch_0.h5\n",
      "data/cifar-10-python-py/batch_1.h5\n",
      "data/cifar-10-python-py/batch_2.h5\n",
      "data/cifar-10-python-py/batch_3.h5\n",
      "data/cifar-10-python-py/batch_4.h5\n",
      "data/cifar-10-python-py/test_batch_0.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "txt_name = 'data/cifar-10-python-py/train_files.txt'\n",
    "with open(txt_name, 'w') as file:\n",
    "    for idx, raw_file in enumerate(train_batchs):\n",
    "        file_path = 'data/cifar-10-python-py/batch_'+str(idx)+'.h5'\n",
    "        print(file_path)\n",
    "        if not os.path.exists(file_path):\n",
    "            data, labels = transform2pointcloud(raw_file)\n",
    "            save_h5_data_label(file_path, data, label)\n",
    "        file.write(file_path)\n",
    "        file.write('\\n')\n",
    "os.chmod(txt_name, 0o777)\n",
    "    \n",
    "\n",
    "txt_name = 'data/cifar-10-python-py/test_files.txt'\n",
    "with open(txt_name, 'w') as file:\n",
    "    for idx, raw_file in enumerate(test_batch):\n",
    "        file_path = 'data/cifar-10-python-py/test_batch_'+str(idx)+'.h5'\n",
    "        print(file_path)\n",
    "        if not os.path.exists(file_path):\n",
    "            data, labels = transform2pointcloud(raw_file)\n",
    "            save_h5_data_label(file_path, data, label)\n",
    "        file.write(file_path)\n",
    "        file.write('\\n')\n",
    "os.chmod(txt_name, 0o777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_DIR = '/notebooks/'\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, 'models'))\n",
    "sys.path.append(os.path.join(BASE_DIR, 'utils'))\n",
    "import cls_provider as provider\n",
    "import os \n",
    "\n",
    "def savePoints(filename, points):\n",
    "    \"\"\"\n",
    "        filename: string asb path\n",
    "        points: npArray nbPts:n\n",
    "    \"\"\"    \n",
    "    with open(filename, 'w') as file:\n",
    "        for row in points:\n",
    "            for idx, col in enumerate(row):\n",
    "                if idx >2:\n",
    "                    file.write(str(int(col))+' ')\n",
    "#                     pass\n",
    "                else:\n",
    "                    file.write(str(col-0.5)+' ')\n",
    "            file.write('\\n')\n",
    "    os.chmod(filename, 0o777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelNet40 official train/test split\n",
    "TRAIN_FILES = provider.getDataFiles( \\\n",
    "    os.path.join(BASE_DIR, 'data/cifar-10-python-py/train_files.txt'))\n",
    "TEST_FILES = provider.getDataFiles(\\\n",
    "    os.path.join(BASE_DIR, 'data/cifar-10-python-py/test_files.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09375    0.125      0.         0.12352943 0.30392158 0.32352942]\n",
      "[9.375e-02 1.250e-01 0.000e+00 1.590e+02 2.050e+02 2.100e+02]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle train files\n",
    "train_file_idxs = np.arange(0, len(TRAIN_FILES))\n",
    "np.random.shuffle(train_file_idxs)\n",
    "\n",
    "NUM_POINT = 1024\n",
    "\n",
    "fn = 1\n",
    "\n",
    "current_data, current_label = provider.loadDataFile(TRAIN_FILES[train_file_idxs[fn]])\n",
    "current_data = current_data[:,0:NUM_POINT,:]\n",
    "current_data, current_label, _ = provider.shuffle_data(current_data, np.squeeze(current_label))            \n",
    "current_label = np.squeeze(current_label)\n",
    "\n",
    "color_image = current_data[0,:,:]\n",
    "print(color_image[100])\n",
    "color_image[:,3:] = (color_image[:,3:] + 0.5) *255\n",
    "print(color_image[100])\n",
    "\n",
    "savePoints('image.txt',color_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graph \n",
    "\n",
    "```\n",
    "\n",
    "model = new Pointnet(\n",
    "    mode, string classification | segmentation\n",
    "    nb_class, int \n",
    "    input_channel, int\n",
    "    nb_pt, int default 1024\n",
    "    base_laerning_rate, float default 0.001\n",
    "    gpu_idx, int default 0,\n",
    "    momentum, float default 0.9\n",
    "    decay_step, int 200000\n",
    "    decay_rate, float 0.7\n",
    "    log_dir, string default logs\n",
    "    bn_init_decay, float default 0.5\n",
    "    bn_decay_decay_rate, float 0.5\n",
    "    bn_decay_decay_step, float 200000\n",
    "    bn_decay_clip, float 0.9\n",
    ")\n",
    "\n",
    "model.train(\n",
    "files, string[]\n",
    "batch_size, int default 32\n",
    ")\n",
    "\n",
    "model.test(\n",
    "files, string[]\n",
    "batch_size, int default 32\n",
    ")\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "INFO:tensorflow:Summary name classify loss is illegal; using classify_loss instead.\n",
      "INFO:tensorflow:Summary name mat loss is illegal; using mat_loss instead.\n"
     ]
    }
   ],
   "source": [
    "from Pointnet import Pointnet\n",
    "import tensorflow as tf\n",
    "import cls_provider as provider\n",
    "import os\n",
    "\n",
    "tf.reset_default_graph()\n",
    "ptnet = Pointnet('classification', 10, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** EPOCH 000 ****\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "config.log_device_placement = False\n",
    "sess = tf.Session(config=config)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(init, {ptnet.end_points['is_training_pl']: True})\n",
    "ptnet.create_log(sess)\n",
    "\n",
    "BASE_DIR = '/notebooks/'\n",
    "\n",
    "# ModelNet40 official train/test split\n",
    "TRAIN_FILES = provider.getDataFiles( \\\n",
    "    os.path.join(BASE_DIR, 'data/cifar-10-python-py/train_files.txt'))\n",
    "TEST_FILES = provider.getDataFiles(\\\n",
    "    os.path.join(BASE_DIR, 'data/cifar-10-python-py/test_files.txt'))\n",
    "\n",
    "MAX_EPOCH = 250\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    print('**** EPOCH %03d ****' % (epoch))\n",
    "    \n",
    "    ptnet.train(sess, TRAIN_FILES)\n",
    "    ptnet.test(sess, TEST_FILES)\n",
    "\n",
    "    # Save the variables to disk.\n",
    "    if epoch % 10 == 0:\n",
    "        save_path = ptnet.saver.save(sess, os.path.join('log', \"model.ckpt\"))\n",
    "        print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,2,3,4,5,6]])\n",
    "a[0,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
