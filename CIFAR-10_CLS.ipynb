{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointnet - CIFAR 10\n",
    "\n",
    "we will using cifar-10 as our dataset to do the classfication work\n",
    "\n",
    "Since the image are 1024(32x32) point only, we will choosing 512 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets\n",
    "\n",
    "### 1. Downdload datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "BASE_DIR = '/notebooks/'\n",
    "# Download dataset for point cloud classification\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "if not os.path.exists(os.path.join(DATA_DIR, 'cifar-10-python-py')):\n",
    "    www = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "\n",
    "    zipfile = os.path.basename(www)\n",
    "    os.system('wget --no-check-certificate  %s; tar -xzf %s' % (www, zipfile))\n",
    "    os.system('mv cifar-10-batches-py %s' % DATA_DIR+'/cifar-10-python-py')\n",
    "    os.system('rm cifar-10-python.tar.gz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. prepare h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Write numpy array data and label to h5_filename\n",
    "def save_h5_data_label(h5_filename, data, label, data_dtype='float32', label_dtype='uint8'):\n",
    "    h5_fout = h5py.File(h5_filename)\n",
    "    h5_fout.create_dataset(\n",
    "            'data', data=data,\n",
    "            compression='gzip', compression_opts=4,\n",
    "            dtype=data_dtype)\n",
    "    h5_fout.create_dataset(\n",
    "            'label', data=label,\n",
    "            compression='gzip', compression_opts=1,\n",
    "            dtype=label_dtype)\n",
    "    h5_fout.close()\n",
    "\n",
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batchs = [\n",
    "    'data/cifar-10-python-py/data_batch_1',\n",
    "    'data/cifar-10-python-py/data_batch_2',\n",
    "    'data/cifar-10-python-py/data_batch_3',\n",
    "    'data/cifar-10-python-py/data_batch_4',\n",
    "    'data/cifar-10-python-py/data_batch_5',\n",
    "]\n",
    "\n",
    "test_batch = [\n",
    "    'data/cifar-10-python-py/test_batch',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform2pointcloud(file_path):\n",
    "    \"\"\"\n",
    "    transfrom iamge to cloud points\n",
    "    \"\"\"\n",
    "    pickle = unpickle(file_path)\n",
    "    label = pickle['labels']\n",
    "    data = pickle['data']\n",
    "    \n",
    "    data_arr = data.reshape([-1, 3, 32, 32])\n",
    "    data_arr = np.transpose(data_arr, (0, 2, 3, 1))\n",
    "    data_arr = data_arr.astype(np.float32)\n",
    "    # normalize the data with -0.5, 0.5\n",
    "    data_arr = data_arr/255. - 0.5\n",
    "    data_arr_size = data_arr.shape\n",
    "    \n",
    "    data = np.zeros((data_arr_size[0], data_arr_size[1], data_arr_size[2], 2+3), np.float32)\n",
    "    data[:,:,:,-3:] = data_arr\n",
    "    \n",
    "    index_arr = np.array(list(np.ndindex((32,32)))).reshape((32,32,2)).astype(np.float32)/32 - 0.5\n",
    "    index_arr = np.expand_dims(index_arr, axis=0)\n",
    "    index_arr = np.repeat(index_arr, [data_arr_size[0]], axis=0)\n",
    "    data[:,:,:,0:2] = index_arr  \n",
    "    data = data.reshape((data_arr_size[0], data_arr_size[1]*data_arr_size[2],  (2+3)))\n",
    "    \n",
    "    return data, np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10000, 1024, 5), (10000,))\n",
      "(dtype('float32'), dtype('int64'))\n",
      "('x,y,z,r,g,b', -0.46875)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "data, label = transform2pointcloud(train_batchs[0])\n",
    "print(data.shape, label.shape)\n",
    "print(data.dtype, label.dtype)\n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.imshow(data[4])\n",
    "print('x,y,z,r,g,b', data[4,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cifar-10-python-py/batch_0.h5\n",
      "data/cifar-10-python-py/batch_1.h5\n",
      "data/cifar-10-python-py/batch_2.h5\n",
      "data/cifar-10-python-py/batch_3.h5\n",
      "data/cifar-10-python-py/batch_4.h5\n",
      "data/cifar-10-python-py/test_batch_0.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "txt_name = 'data/cifar-10-python-py/train_files.txt'\n",
    "with open(txt_name, 'w') as file:\n",
    "    for idx, raw_file in enumerate(train_batchs):\n",
    "        file_path = 'data/cifar-10-python-py/batch_'+str(idx)+'.h5'\n",
    "        print(file_path)\n",
    "        if not os.path.exists(file_path):\n",
    "            data, labels = transform2pointcloud(raw_file)\n",
    "            save_h5_data_label(file_path, data, label)\n",
    "        file.write(file_path)\n",
    "        file.write('\\n')\n",
    "os.chmod(txt_name, 0o777)\n",
    "    \n",
    "\n",
    "txt_name = 'data/cifar-10-python-py/test_files.txt'\n",
    "with open(txt_name, 'w') as file:\n",
    "    for idx, raw_file in enumerate(test_batch):\n",
    "        file_path = 'data/cifar-10-python-py/test_batch_'+str(idx)+'.h5'\n",
    "        print(file_path)\n",
    "        if not os.path.exists(file_path):\n",
    "            data, labels = transform2pointcloud(raw_file)\n",
    "            save_h5_data_label(file_path, data, label)\n",
    "        file.write(file_path)\n",
    "        file.write('\\n')\n",
    "os.chmod(txt_name, 0o777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_DIR = '/notebooks/'\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, 'models'))\n",
    "sys.path.append(os.path.join(BASE_DIR, 'utils'))\n",
    "import cls_provider as provider\n",
    "import os \n",
    "\n",
    "def savePoints(filename, points):\n",
    "    \"\"\"\n",
    "        filename: string asb path\n",
    "        points: npArray nbPts:n\n",
    "    \"\"\"    \n",
    "    with open(filename, 'w') as file:\n",
    "        for row in points:\n",
    "            for idx, col in enumerate(row):\n",
    "                if idx >2:\n",
    "                    file.write(str(int(col))+' ')\n",
    "#                     pass\n",
    "                else:\n",
    "                    file.write(str(col-0.5)+' ')\n",
    "            file.write('\\n')\n",
    "    os.chmod(filename, 0o777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelNet40 official train/test split\n",
    "TRAIN_FILES = provider.getDataFiles( \\\n",
    "    os.path.join(BASE_DIR, 'data/cifar-10-python-py/train_files.txt'))\n",
    "TEST_FILES = provider.getDataFiles(\\\n",
    "    os.path.join(BASE_DIR, 'data/cifar-10-python-py/test_files.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.40625    -0.375       0.08431375  0.14313728  0.19411767]\n",
      "[-4.062500e-01 -3.750000e-01  8.431375e-02  1.640000e+02  1.770000e+02]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle train files\n",
    "train_file_idxs = np.arange(0, len(TRAIN_FILES))\n",
    "np.random.shuffle(train_file_idxs)\n",
    "\n",
    "NUM_POINT = 1024\n",
    "\n",
    "fn = 1\n",
    "\n",
    "current_data, current_label = provider.loadDataFile(TRAIN_FILES[train_file_idxs[fn]])\n",
    "current_data = current_data[:,0:NUM_POINT,:]\n",
    "current_data, current_label, _ = provider.shuffle_data(current_data, np.squeeze(current_label))            \n",
    "current_label = np.squeeze(current_label)\n",
    "\n",
    "color_image = current_data[0,:,:]\n",
    "print(color_image[100])\n",
    "color_image[:,3:] = (color_image[:,3:] + 0.5) *255\n",
    "print(color_image[100])\n",
    "\n",
    "savePoints('image.txt',color_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graph \n",
    "\n",
    "```\n",
    "\n",
    "model = new Pointnet(\n",
    "    mode, string classification | segmentation\n",
    "    nb_class, int \n",
    "    input_channel, int\n",
    "    nb_pt, int default 1024\n",
    "    base_laerning_rate, float default 0.001\n",
    "    gpu_idx, int default 0,\n",
    "    momentum, float default 0.9\n",
    "    decay_step, int 200000\n",
    "    decay_rate, float 0.7\n",
    "    log_dir, string default logs\n",
    "    bn_init_decay, float default 0.5\n",
    "    bn_decay_decay_rate, float 0.5\n",
    "    bn_decay_decay_step, float 200000\n",
    "    bn_decay_clip, float 0.9\n",
    ")\n",
    "\n",
    "model.train(\n",
    "files, string[]\n",
    "batch_size, int default 32\n",
    ")\n",
    "\n",
    "model.test(\n",
    "files, string[]\n",
    "batch_size, int default 32\n",
    ")\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name classify loss is illegal; using classify_loss instead.\n",
      "INFO:tensorflow:Summary name mat loss is illegal; using mat_loss instead.\n"
     ]
    }
   ],
   "source": [
    "from Pointnet import Pointnet\n",
    "import tensorflow as tf\n",
    "import cls_provider as provider\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "tf.reset_default_graph()\n",
    "ptnet = Pointnet('classification', 40, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** EPOCH 000 ****\n",
      "mean loss: 655.303208\n",
      "accuracy: 0.657715\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 6617.013383\n",
      "eval accuracy: 0.371753\n",
      "eval avg class acc: 0.260097\n",
      "Model saved in file: log/model.ckpt\n",
      "**** EPOCH 001 ****\n",
      "mean loss: 1836.734909\n",
      "accuracy: 0.720703\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 140782.994630\n",
      "eval accuracy: 0.359984\n",
      "eval avg class acc: 0.261337\n",
      "**** EPOCH 002 ****\n",
      "mean loss: 4025.577370\n",
      "accuracy: 0.741699\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 82267.954428\n",
      "eval accuracy: 0.585227\n",
      "eval avg class acc: 0.447585\n",
      "**** EPOCH 003 ****\n",
      "mean loss: 8568.304924\n",
      "accuracy: 0.755371\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 174554.706397\n",
      "eval accuracy: 0.630276\n",
      "eval avg class acc: 0.486901\n",
      "**** EPOCH 004 ****\n",
      "mean loss: 15265.199905\n",
      "accuracy: 0.776367\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 330604.854036\n",
      "eval accuracy: 0.663149\n",
      "eval avg class acc: 0.529298\n",
      "**** EPOCH 005 ****\n",
      "mean loss: 25933.711972\n",
      "accuracy: 0.810662\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 649106.910600\n",
      "eval accuracy: 0.696429\n",
      "eval avg class acc: 0.566481\n",
      "**** EPOCH 006 ****\n",
      "mean loss: 42941.231262\n",
      "accuracy: 0.806641\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 910904.215199\n",
      "eval accuracy: 0.715503\n",
      "eval avg class acc: 0.599508\n",
      "**** EPOCH 007 ****\n",
      "mean loss: 60725.594788\n",
      "accuracy: 0.810059\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 963144.889509\n",
      "eval accuracy: 0.757711\n",
      "eval avg class acc: 0.641890\n",
      "**** EPOCH 008 ****\n",
      "mean loss: 87209.062012\n",
      "accuracy: 0.829590\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 1838582.852374\n",
      "eval accuracy: 0.724432\n",
      "eval avg class acc: 0.614897\n",
      "**** EPOCH 009 ****\n",
      "mean loss: 125967.958130\n",
      "accuracy: 0.832520\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 3882586.456169\n",
      "eval accuracy: 0.762175\n",
      "eval avg class acc: 0.657649\n",
      "**** EPOCH 010 ****\n",
      "mean loss: 178846.247070\n",
      "accuracy: 0.833008\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 3095604.831372\n",
      "eval accuracy: 0.766234\n",
      "eval avg class acc: 0.673718\n",
      "Model saved in file: log/model.ckpt\n",
      "**** EPOCH 011 ****\n",
      "mean loss: 244924.594482\n",
      "accuracy: 0.847168\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 8400995.314935\n",
      "eval accuracy: 0.761364\n",
      "eval avg class acc: 0.652079\n",
      "**** EPOCH 012 ****\n",
      "mean loss: 328951.722656\n",
      "accuracy: 0.838867\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 5737374.873782\n",
      "eval accuracy: 0.794643\n",
      "eval avg class acc: 0.695178\n",
      "**** EPOCH 013 ****\n",
      "mean loss: 404760.040441\n",
      "accuracy: 0.851716\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 9206366.154627\n",
      "eval accuracy: 0.816153\n",
      "eval avg class acc: 0.719545\n",
      "**** EPOCH 014 ****\n",
      "mean loss: 520583.417969\n",
      "accuracy: 0.831543\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 11960559.198458\n",
      "eval accuracy: 0.782873\n",
      "eval avg class acc: 0.689403\n",
      "**** EPOCH 015 ****\n",
      "mean loss: 673091.279297\n",
      "accuracy: 0.857910\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 19142129.709416\n",
      "eval accuracy: 0.817370\n",
      "eval avg class acc: 0.729898\n",
      "**** EPOCH 016 ****\n",
      "mean loss: 840841.178711\n",
      "accuracy: 0.877930\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 17162321.775162\n",
      "eval accuracy: 0.841721\n",
      "eval avg class acc: 0.758624\n",
      "**** EPOCH 017 ****\n",
      "mean loss: 927498.899414\n",
      "accuracy: 0.876465\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 18083240.482955\n",
      "eval accuracy: 0.843750\n",
      "eval avg class acc: 0.764553\n",
      "**** EPOCH 018 ****\n",
      "mean loss: 1080215.592773\n",
      "accuracy: 0.879395\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 24605717.202922\n",
      "eval accuracy: 0.852679\n",
      "eval avg class acc: 0.784638\n",
      "**** EPOCH 019 ****\n",
      "mean loss: 1241322.533203\n",
      "accuracy: 0.879395\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 32003877.422078\n",
      "eval accuracy: 0.854708\n",
      "eval avg class acc: 0.784459\n",
      "**** EPOCH 020 ****\n",
      "mean loss: 1333117.398438\n",
      "accuracy: 0.880371\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 38752554.897727\n",
      "eval accuracy: 0.855925\n",
      "eval avg class acc: 0.781473\n",
      "Model saved in file: log/model.ckpt\n",
      "**** EPOCH 021 ****\n",
      "mean loss: 1576767.421875\n",
      "accuracy: 0.886719\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 40561472.355519\n",
      "eval accuracy: 0.866071\n",
      "eval avg class acc: 0.787404\n",
      "**** EPOCH 022 ****\n",
      "mean loss: 1741070.409314\n",
      "accuracy: 0.890931\n",
      "----0-----\n",
      "----1-----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-318246033222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mptnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_FILES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mptnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_FILES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Save the variables to disk.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/Pointnet.pyc\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, sess, files)\u001b[0m\n\u001b[1;32m    563\u001b[0m                                                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                                                                 ], \n\u001b[0;32m--> 565\u001b[0;31m                                                                 feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m                 \u001b[0mpred_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "config.log_device_placement = False\n",
    "sess = tf.Session(config=config)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(init, {ptnet.end_points['is_training_pl']: True})\n",
    "ptnet.create_log(sess)\n",
    "\n",
    "BASE_DIR = '/notebooks/'\n",
    "\n",
    "# ModelNet40 official train/test split\n",
    "# modelnet40_ply_hdf5_2048\n",
    "# \n",
    "TRAIN_FILES = provider.getDataFiles( \\\n",
    "    os.path.join(BASE_DIR, 'data/modelnet40_ply_hdf5_2048/train_files.txt'))\n",
    "TEST_FILES = provider.getDataFiles(\\\n",
    "    os.path.join(BASE_DIR, 'data/modelnet40_ply_hdf5_2048/test_files.txt'))\n",
    "\n",
    "MAX_EPOCH = 25\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    print('**** EPOCH %03d ****' % (epoch))\n",
    "    \n",
    "    ptnet.train(sess, TRAIN_FILES)\n",
    "    ptnet.test(sess, TEST_FILES)\n",
    "\n",
    "    # Save the variables to disk.\n",
    "    if epoch % 10 == 0:\n",
    "        save_path = ptnet.saver.save(sess, os.path.join('log', \"model.ckpt\"))\n",
    "        print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
