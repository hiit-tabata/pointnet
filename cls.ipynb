{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointnet - Classification \n",
    "\n",
    "This is a re-imeplmentation of poinnet. \n",
    "There are some code are the same with the original code, such as provide etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "BASE_DIR = '/notebooks/'\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, 'models'))\n",
    "sys.path.append(os.path.join(BASE_DIR, 'utils'))\n",
    "import cls_provider as provider\n",
    "import pointnet_cls as MODEL\n",
    "# import tf_util\n",
    "# import pointnet_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_POINT = 1024\n",
    "MAX_EPOCH = 250\n",
    "BASE_LEARNING_RATE = 0.001\n",
    "GPU_INDEX = 0\n",
    "MOMENTUM = 0.9\n",
    "OPTIMIZER = 'adam'\n",
    "DECAY_STEP = 200000\n",
    "DECAY_RATE = 0.7\n",
    "LOG_DIR = 'logs'\n",
    "\n",
    "\n",
    "MAX_NUM_POINT = 2048\n",
    "NUM_CLASSES = 40\n",
    "\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
    "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
    "\n",
    "\n",
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelNet40 train/test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelNet40 official train/test split\n",
    "TRAIN_FILES = provider.getDataFiles( \\\n",
    "    os.path.join(BASE_DIR, 'data/modelnet40_ply_hdf5_2048/train_files.txt'))\n",
    "TEST_FILES = provider.getDataFiles(\\\n",
    "    os.path.join(BASE_DIR, 'data/modelnet40_ply_hdf5_2048/test_files.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train rate/ decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_learning_rate(batch):\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                        DECAY_STEP,          # Decay step.\n",
    "                        DECAY_RATE,          # Decay rate.\n",
    "                        staircase=True)\n",
    "    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
    "    return learning_rate        \n",
    "\n",
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.train.exponential_decay(\n",
    "                      BN_INIT_DECAY,\n",
    "                      batch*BATCH_SIZE,\n",
    "                      BN_DECAY_DECAY_STEP,\n",
    "                      BN_DECAY_DECAY_RATE,\n",
    "                      staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph():\n",
    "    pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "    is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "    print(is_training_pl)\n",
    "\n",
    "    # Note the global_step=batch parameter to minimize. \n",
    "    # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "    batch = tf.Variable(0)\n",
    "    bn_decay = get_bn_decay(batch)\n",
    "    tf.summary.scalar('bn_decay', bn_decay)\n",
    "\n",
    "    # Get model and loss \n",
    "    pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "    loss = MODEL.get_loss(pred, labels_pl, end_points)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    correct = tf.equal(tf.argmax(pred, 1), tf.to_int64(labels_pl))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # Get training operator\n",
    "    learning_rate = get_learning_rate(batch)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    if OPTIMIZER == 'momentum':\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "    elif OPTIMIZER == 'adam':\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step=batch)\n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    return {\n",
    "        'pointclouds_pl':pointclouds_pl,\n",
    "        'labels_pl':labels_pl,\n",
    "        'is_training_pl':is_training_pl,\n",
    "        'train_op':train_op,\n",
    "        'batch':batch,\n",
    "        'bn_decay':bn_decay,\n",
    "        'pred':pred,\n",
    "        'end_points':end_points,\n",
    "        'loss':loss,\n",
    "        'correct':correct,\n",
    "        'learning_rate':learning_rate,\n",
    "        'train_op':train_op,\n",
    "        'saver':saver\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(sess, ops, train_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = True\n",
    "    \n",
    "    # Shuffle train files\n",
    "    train_file_idxs = np.arange(0, len(TRAIN_FILES))\n",
    "    np.random.shuffle(train_file_idxs)\n",
    "    \n",
    "    for fn in range(len(TRAIN_FILES)):\n",
    "        log_string('----' + str(fn) + '-----')\n",
    "        current_data, current_label = provider.loadDataFile(TRAIN_FILES[train_file_idxs[fn]])\n",
    "        current_data = current_data[:,0:NUM_POINT,:]\n",
    "        current_data, current_label, _ = provider.shuffle_data(current_data, np.squeeze(current_label))            \n",
    "        current_label = np.squeeze(current_label)\n",
    "        \n",
    "        file_size = current_data.shape[0]\n",
    "        num_batches = file_size // BATCH_SIZE\n",
    "        \n",
    "        total_correct = 0\n",
    "        total_seen = 0\n",
    "        loss_sum = 0\n",
    "       \n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "            \n",
    "            # Augment batched point clouds by rotation and jittering\n",
    "            rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
    "            jittered_data = provider.jitter_point_cloud(rotated_data)\n",
    "            feed_dict = {ops['pointclouds_pl']: jittered_data,\n",
    "                         ops['labels_pl']: current_label[start_idx:end_idx],\n",
    "                         ops['is_training_pl']: is_training,}\n",
    "            summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                ops['train_op'], ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "            train_writer.add_summary(summary, step)\n",
    "            pred_val = np.argmax(pred_val, 1)\n",
    "            correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "            total_correct += correct\n",
    "            total_seen += BATCH_SIZE\n",
    "            loss_sum += loss_val\n",
    "        \n",
    "        log_string('mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "        log_string('accuracy: %f' % (total_correct / float(total_seen)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_epoch(sess, ops, test_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = False\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    \n",
    "    for fn in range(len(TEST_FILES)):\n",
    "        log_string('----' + str(fn) + '-----')\n",
    "        current_data, current_label = provider.loadDataFile(TEST_FILES[fn])\n",
    "        current_data = current_data[:,0:NUM_POINT,:]\n",
    "        current_label = np.squeeze(current_label)\n",
    "        \n",
    "        file_size = current_data.shape[0]\n",
    "        num_batches = file_size // BATCH_SIZE\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "\n",
    "            feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                         ops['labels_pl']: current_label[start_idx:end_idx],\n",
    "                         ops['is_training_pl']: is_training}\n",
    "            summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "            pred_val = np.argmax(pred_val, 1)\n",
    "            correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "            total_correct += correct\n",
    "            total_seen += BATCH_SIZE\n",
    "            loss_sum += (loss_val*BATCH_SIZE)\n",
    "            for i in range(start_idx, end_idx):\n",
    "                l = current_label[i]\n",
    "                total_seen_class[l] += 1\n",
    "                total_correct_class[l] += (pred_val[i-start_idx] == l)\n",
    "            \n",
    "    log_string('eval mean loss: %f' % (loss_sum / float(total_seen)))\n",
    "    log_string('eval accuracy: %f'% (total_correct / float(total_seen)))\n",
    "    log_string('eval avg class acc: %f' % (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_2:0\", shape=(), dtype=bool, device=/device:GPU:0)\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "INFO:tensorflow:Summary name classify loss is illegal; using classify_loss instead.\n",
      "INFO:tensorflow:Summary name mat loss is illegal; using mat_loss instead.\n",
      "**** EPOCH 000 ****\n",
      "----0-----\n",
      "mean loss: 3.757465\n",
      "accuracy: 0.208496\n",
      "----1-----\n",
      "mean loss: 2.746015\n",
      "accuracy: 0.325980\n",
      "----2-----\n",
      "mean loss: 2.408370\n",
      "accuracy: 0.383789\n",
      "----3-----\n",
      "mean loss: 2.022735\n",
      "accuracy: 0.456543\n",
      "----4-----\n",
      "mean loss: 1.774608\n",
      "accuracy: 0.516113\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 1.583176\n",
      "eval accuracy: 0.540990\n",
      "eval avg class acc: 0.445353\n",
      "Model saved in file: logs/model.ckpt\n",
      "**** EPOCH 001 ****\n",
      "----0-----\n",
      "mean loss: 1.658119\n",
      "accuracy: 0.522949\n",
      "----1-----\n",
      "mean loss: 1.473063\n",
      "accuracy: 0.588867\n",
      "----2-----\n",
      "mean loss: 1.532518\n",
      "accuracy: 0.562500\n",
      "----3-----\n",
      "mean loss: 1.455473\n",
      "accuracy: 0.588235\n",
      "----4-----\n",
      "mean loss: 1.426247\n",
      "accuracy: 0.587891\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 1.121190\n",
      "eval accuracy: 0.674107\n",
      "eval avg class acc: 0.573950\n",
      "**** EPOCH 002 ****\n",
      "----0-----\n",
      "mean loss: 1.496060\n",
      "accuracy: 0.612793\n",
      "----1-----\n",
      "mean loss: 1.744617\n",
      "accuracy: 0.506836\n",
      "----2-----\n",
      "mean loss: 1.449823\n",
      "accuracy: 0.571078\n",
      "----3-----\n",
      "mean loss: 1.309969\n",
      "accuracy: 0.611816\n",
      "----4-----\n",
      "mean loss: 1.217808\n",
      "accuracy: 0.639648\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 1.119142\n",
      "eval accuracy: 0.677354\n",
      "eval avg class acc: 0.597093\n",
      "**** EPOCH 003 ****\n",
      "----0-----\n",
      "mean loss: 1.182029\n",
      "accuracy: 0.654412\n",
      "----1-----\n",
      "mean loss: 1.165375\n",
      "accuracy: 0.653809\n",
      "----2-----\n",
      "mean loss: 1.167319\n",
      "accuracy: 0.641113\n",
      "----3-----\n",
      "mean loss: 1.118672\n",
      "accuracy: 0.661621\n",
      "----4-----\n",
      "mean loss: 1.033824\n",
      "accuracy: 0.693359\n",
      "----0-----\n",
      "----1-----\n",
      "eval mean loss: 1.008175\n",
      "eval accuracy: 0.711445\n",
      "eval avg class acc: 0.661810\n",
      "**** EPOCH 004 ****\n",
      "----0-----\n",
      "mean loss: 0.988324\n",
      "accuracy: 0.715074\n",
      "----1-----\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "    sess_graph = create_graph()\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.allow_soft_placement = True\n",
    "    config.log_device_placement = False\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    # Add summary writers\n",
    "    #merged = tf.merge_all_summaries()\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'),\n",
    "                              sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
    "\n",
    "    # Init variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    # To fix the bug introduced in TF 0.12.1 as in\n",
    "    # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
    "    #sess.run(init)\n",
    "    sess.run(init, {sess_graph['is_training_pl']: True})\n",
    "\n",
    "    ops = {'pointclouds_pl': sess_graph['pointclouds_pl'],\n",
    "           'labels_pl': sess_graph['labels_pl'],\n",
    "           'is_training_pl': sess_graph['is_training_pl'],\n",
    "           'pred': sess_graph['pred'],\n",
    "           'loss': sess_graph['loss'],\n",
    "           'train_op': sess_graph['train_op'],\n",
    "           'merged': merged,\n",
    "           'step': sess_graph['batch']}\n",
    "\n",
    "    for epoch in range(MAX_EPOCH):\n",
    "        log_string('**** EPOCH %03d ****' % (epoch))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        train_one_epoch(sess, ops, train_writer)\n",
    "        eval_one_epoch(sess, ops, test_writer)\n",
    "\n",
    "        # Save the variables to disk.\n",
    "        if epoch % 10 == 0:\n",
    "            save_path = sess_graph['saver'].save(sess, os.path.join(LOG_DIR, \"model.ckpt\"))\n",
    "            log_string(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FOUT.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
